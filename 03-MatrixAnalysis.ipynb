{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c9944b-fbfc-4d39-9e73-289b478a6af4",
   "metadata": {},
   "source": [
    "# MHD matrix analysis\n",
    "This code performs the cluster analysis to a distance matrix. It tries different combinations of parameters to find the best combination based on a mix of criteria.\n",
    "\n",
    "We load the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a83a80-dd21-4e4a-a9c8-4e750f5e1fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from scipy.ndimage import sobel\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances_argmin_min\n",
    "import shutil\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from umap import UMAP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5e5a9-01ac-4c15-aff5-a2f98a2f15ed",
   "metadata": {},
   "source": [
    "Plotting the distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08977d71-a1f3-4f91-b4db-ba15f948d51a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distance_matrix = np.load(\"Z:/data/test/blocks/final_distance_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e81326-a8bf-43a1-aa9a-6d6a5f199ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the matrix\n",
    "plt.magma()\n",
    "plt.imshow(distance_matrix)\n",
    "plt.title('MHD Matrix between all the pictures of the experiment', size = 10)\n",
    "plt.colorbar(format='%.1f', label='Modified Hausdorff distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb3e31-dd58-4727-b89e-3f216fdff200",
   "metadata": {},
   "source": [
    "## General data overview \n",
    "### The images that least resemble each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45a4a6c-51b5-49e8-b4c9-1e753f813ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a0c208-9172-4d83-83ca-08eed2408c62",
   "metadata": {},
   "source": [
    "We will plot those pairs of photos that least resemble each other.  \n",
    "\n",
    "We need to specify where are the original photos and the number of pairs to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8dc4a-e0ed-417b-82ea-b56fdbbce7cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parámetros de entrada\n",
    "images_folder = 'Z:\\data\\exp20\\deepest_all_every10'\n",
    "num_pairs = 2  # Número de pares de imágenes a visualizar (cambiar según necesites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c5ad97-0a68-4f5e-98bc-a5f2365e8148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Asegurarse de que la matriz es cuadrada (matriz de distancias simétrica)\n",
    "assert distance_matrix.shape[0] == distance_matrix.shape[1], \"La matriz de distancias no es cuadrada.\"\n",
    "\n",
    "# Establecer las distancias de la diagonal principal a -1 para ignorarlas (ya que son 0, misma imagen)\n",
    "np.fill_diagonal(distance_matrix, -1)\n",
    "\n",
    "# Obtener todos los nombres de los archivos de imagen en la carpeta y ordenarlos alfabéticamente\n",
    "image_files = sorted([f for f in os.listdir(images_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Asegurarse de que el número de imágenes coincide con la dimensión de la matriz de distancias\n",
    "assert len(image_files) == distance_matrix.shape[0], \"El número de imágenes no coincide con la dimensión de la matriz de distancias.\"\n",
    "\n",
    "# Usar un heap para encontrar las mayores distancias\n",
    "largest_distances = []  # Heap para las mayores distancias\n",
    "for i in range(distance_matrix.shape[0]):\n",
    "    for j in range(i + 1, distance_matrix.shape[1]):  # Solo recorremos la mitad superior de la matriz (matriz simétrica)\n",
    "        distance = distance_matrix[i, j]\n",
    "        if len(largest_distances) < num_pairs:\n",
    "            heapq.heappush(largest_distances, (distance, i, j))\n",
    "        else:\n",
    "            heapq.heappushpop(largest_distances, (distance, i, j))\n",
    "\n",
    "# Ordenar los pares de imágenes por la distancia en orden descendente\n",
    "largest_distances.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "# Plotear las imágenes de los pares más diferentes\n",
    "fig, axes = plt.subplots(num_pairs, 2, figsize=(10, 5 * num_pairs))\n",
    "for i, (dist_value, index_1, index_2) in enumerate(largest_distances):\n",
    "    # Obtener los nombres de las imágenes basados en los índices\n",
    "    image_1_name = image_files[index_1]\n",
    "    image_2_name = image_files[index_2]\n",
    "    \n",
    "    # Construir las rutas completas de las imágenes\n",
    "    image_1_path = os.path.join(images_folder, image_1_name)\n",
    "    image_2_path = os.path.join(images_folder, image_2_name)\n",
    "    \n",
    "    # Cargar las imágenes\n",
    "    image_1 = Image.open(image_1_path)\n",
    "    image_2 = Image.open(image_2_path)\n",
    "    \n",
    "    # Mostrar las imágenes en el gráfico\n",
    "    axes[i, 0].imshow(image_1)\n",
    "    axes[i, 0].set_title(f'{image_1_name} (Índice {index_1})')\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(image_2)\n",
    "    axes[i, 1].set_title(f'{image_2_name} (Índice {index_2})')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Mostrar la distancia correspondiente\n",
    "    print(f\"Pares de imágenes: {image_1_name} (Índice {index_1}) y {image_2_name} (Índice {index_2}) - Distancia: {dist_value}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0369dd-fc55-43b5-bfc6-aab5c99f53a7",
   "metadata": {},
   "source": [
    "## UMAP embedding + clustering with both HDBSCAN & DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a3a0b7-c72e-4c85-859a-39b146aca13e",
   "metadata": {},
   "source": [
    "We will perform dimensionality reduction using UMAP and then apply clustering algorithms (HDBSCAN and DBSCAN) to the embedded data. The results will be saved in a structured directory as shown below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139ae74-6185-4da2-ba49-e77fae3c08d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "```raw\n",
    "Z:/data/test/results/\n",
    "├── UMAP_nn10_md0.1_nc5/\n",
    "│   ├── embedding.npy\n",
    "│   ├── HDBSCAN_mcs10_msNone/\n",
    "│   │   ├── cluster_labels.npy\n",
    "│   │   ├── centroids/\n",
    "│   │       ├── cluster_0_centroid.png\n",
    "│   │       └── ...\n",
    "│   ├── DBSCAN_eps0.5_ms15/\n",
    "│   │   ├── cluster_labels.npy\n",
    "│   │   ├── centroids/\n",
    "│   │       └── ...\n",
    "│   └── ...\n",
    "└── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab6233e-68aa-49b0-9dbb-6667d44acb7f",
   "metadata": {},
   "source": [
    "Specify the base directory for saving the results and the directory containing the original images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26abc61-8809-4243-95e8-05a2e1c193c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio base para guardar los resultados\n",
    "results_base_dir = \"Z:/data/test/results-every10\"\n",
    "# Directorio donde se encuentran las imágenes originales\n",
    "images_directory = \"E:/Exp20/OPTdeepest_every10\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4564c7b7-0742-4efd-a8cd-878bed66a46d",
   "metadata": {},
   "source": [
    "Define the parameter ranges for UMAP and the clustering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e108fde2-f7c9-4aaa-9bbb-94e15448edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros ajustados\n",
    "umap_n_neighbors = [10, 30, 50]\n",
    "umap_min_dist = [0.0, 0.1, 0.3]\n",
    "umap_n_components = [2, 5, 10, 20]  # Nuevas dimensiones\n",
    "\n",
    "hdbscan_min_cluster_size = [10, 20, 50]\n",
    "hdbscan_min_samples = [None, 5, 15]\n",
    "\n",
    "dbscan_eps = [0.2, 0.5, 0.8]\n",
    "dbscan_min_samples = [5, 15, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e38a76c1-2cbc-4fa8-86ee-49d98c0f83df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import additional necessary packages\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from scipy.spatial import ConvexHull\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "\n",
    "\n",
    "# Manejar los valores NaN en la matriz\n",
    "max_distance = np.nanmax(distance_matrix[np.isfinite(distance_matrix)])\n",
    "distance_matrix_filled = np.nan_to_num(distance_matrix, nan=max_distance)\n",
    "\n",
    "# Índices de los puntos (para colorear según el tiempo)\n",
    "indices = np.arange(distance_matrix.shape[0])\n",
    "\n",
    "image_files = sorted([f for f in os.listdir(images_directory) if f.endswith('.png') or f.endswith('.jpg')])\n",
    "\n",
    "# Crear combinaciones de parámetros\n",
    "umap_params = list(itertools.product(umap_n_neighbors, umap_min_dist, umap_n_components))\n",
    "hdbscan_params = list(itertools.product(hdbscan_min_cluster_size, hdbscan_min_samples))\n",
    "dbscan_params = list(itertools.product(dbscan_eps, dbscan_min_samples))\n",
    "\n",
    "\n",
    "# Asegurarse de que el directorio base existe\n",
    "os.makedirs(results_base_dir, exist_ok=True)\n",
    "\n",
    "# Lista para almacenar métricas de clustering\n",
    "metrics_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d787fb-6f8a-4717-9aba-eeecdc364a06",
   "metadata": {},
   "source": [
    "### UMAP Dimensionality Reduction\n",
    "We iterate over all combinations of UMAP parameters to reduce the dimensionality of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c296e10-5059-4325-b732-e05dfc5789c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3954806918.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Import additional necessary packages\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Iterar sobre todas las combinaciones de parámetros de UMAP\n",
    "for n_neighbors, min_dist, n_components in umap_params:\n",
    "    print(f\"Procesando UMAP con n_neighbors={n_neighbors}, min_dist={min_dist}, n_components={n_components}\")\n",
    "    # Realizar la proyección UMAP con los parámetros actuales\n",
    "    reducer = umap.UMAP(\n",
    "        n_components=n_components,\n",
    "        metric='precomputed',\n",
    "        random_state=42,\n",
    "        min_dist=min_dist,\n",
    "        n_neighbors=n_neighbors\n",
    "    )\n",
    "    embedding = reducer.fit_transform(distance_matrix_filled)\n",
    "    \n",
    "    # Normalizar el embedding para mejorar el clustering\n",
    "    scaler = StandardScaler()\n",
    "    embedding_scaled = scaler.fit_transform(embedding)\n",
    "    \n",
    "    # Crear directorio para esta combinación de parámetros UMAP\n",
    "    umap_dir_name = f\"UMAP_nn{n_neighbors}_md{min_dist}_nc{n_components}\"\n",
    "    umap_dir = os.path.join(results_base_dir, umap_dir_name)\n",
    "    os.makedirs(umap_dir, exist_ok=True)\n",
    "    \n",
    "    # Guardar el embedding UMAP\n",
    "    np.save(os.path.join(umap_dir, \"embedding.npy\"), embedding)\n",
    "    \n",
    "    # -----------------------\n",
    "    # Clustering con HDBSCAN\n",
    "    # -----------------------\n",
    "    # We apply DBSCAN clustering on the UMAP-reduced data using different parameter combinations.\n",
    "    \n",
    "    for min_cluster_size, min_samples in hdbscan_params:\n",
    "        print(f\"  Procesando HDBSCAN con min_cluster_size={min_cluster_size}, min_samples={min_samples}\")\n",
    "        # Aplicar HDBSCAN con los parámetros actuales\n",
    "        clusterer_hdbscan = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples\n",
    "        )\n",
    "        cluster_labels_hdbscan = clusterer_hdbscan.fit_predict(embedding_scaled)\n",
    "        \n",
    "        # Calcular métricas de clustering\n",
    "        num_clusters = len(set(cluster_labels_hdbscan)) - (1 if -1 in cluster_labels_hdbscan else 0)\n",
    "        \n",
    "        if num_clusters > 1:\n",
    "            silhouette_avg = silhouette_score(embedding_scaled[cluster_labels_hdbscan != -1],\n",
    "                                              cluster_labels_hdbscan[cluster_labels_hdbscan != -1])\n",
    "        else:\n",
    "            silhouette_avg = np.nan\n",
    "\n",
    "        if num_clusters > 1:\n",
    "            davies_bouldin = davies_bouldin_score(embedding_scaled[cluster_labels != -1], \n",
    "                                                  cluster_labels[cluster_labels != -1])\n",
    "        else:\n",
    "            davies_bouldin = np.nan\n",
    "\n",
    "        if num_clusters > 1:\n",
    "            calinski_harabasz = calinski_harabasz_score(embedding_scaled[cluster_labels != -1], \n",
    "                                                        cluster_labels[cluster_labels != -1])\n",
    "        else:\n",
    "            calinski_harabasz = np.nan\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Guardar métricas en la lista\n",
    "        metrics_list.append({\n",
    "            'UMAP_n_neighbors': n_neighbors,\n",
    "            'UMAP_min_dist': min_dist,\n",
    "            'UMAP_n_components': n_components,\n",
    "            'Algorithm': 'HDBSCAN',\n",
    "            'Params': f\"min_cluster_size={min_cluster_size}, min_samples={min_samples}\",\n",
    "            'Num_Clusters': num_clusters,\n",
    "            'Silhouette_Score': silhouette_avg,\n",
    "            'Davies_Bouldin_Index': davies_bouldin,\n",
    "            'Calinski_Harabasz_Index': calinski_harabasz\n",
    "        })\n",
    "        \n",
    "        # Crear directorio para esta combinación de parámetros HDBSCAN\n",
    "        hdbscan_dir_name = f\"HDBSCAN_mcs{min_cluster_size}_ms{min_samples}\"\n",
    "        hdbscan_dir = os.path.join(umap_dir, hdbscan_dir_name)\n",
    "        os.makedirs(hdbscan_dir, exist_ok=True)\n",
    "        \n",
    "        # Guardar las etiquetas de cluster\n",
    "        np.save(os.path.join(hdbscan_dir, \"cluster_labels.npy\"), cluster_labels_hdbscan)\n",
    "        \n",
    "        # Si n_components es 2, podemos generar gráficos\n",
    "        if n_components == 2:\n",
    "            # Visualizar y guardar el gráfico sin mostrarlo\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            # Graficar los puntos de ruido por separado\n",
    "            noise_mask_hdbscan = cluster_labels_hdbscan == -1\n",
    "            plt.scatter(\n",
    "                embedding[noise_mask_hdbscan, 0],\n",
    "                embedding[noise_mask_hdbscan, 1],\n",
    "                c='black',\n",
    "                s=10,\n",
    "                alpha=0.5,\n",
    "                label='Ruido'\n",
    "            )\n",
    "            \n",
    "            # Graficar los puntos asignados a clusters\n",
    "            cluster_mask_hdbscan = ~noise_mask_hdbscan\n",
    "            scatter_hdbscan = plt.scatter(\n",
    "                embedding[cluster_mask_hdbscan, 0],\n",
    "                embedding[cluster_mask_hdbscan, 1],\n",
    "                c=indices[cluster_mask_hdbscan],\n",
    "                cmap='viridis',\n",
    "                s=30,\n",
    "                alpha=0.8,\n",
    "                label='Datos'\n",
    "            )\n",
    "            plt.colorbar(scatter_hdbscan, label='Índice (Tiempo)')\n",
    "            \n",
    "            # Generar y dibujar los polígonos para los clusters\n",
    "            unique_clusters_hdbscan = np.unique(cluster_labels_hdbscan)\n",
    "            unique_clusters_hdbscan = unique_clusters_hdbscan[unique_clusters_hdbscan != -1]  # Excluir el ruido\n",
    "            \n",
    "            # Configuración del polígono\n",
    "            polygon_color = 'lightgray'\n",
    "            polygon_alpha = 0.5\n",
    "            \n",
    "            for cluster_label in unique_clusters_hdbscan:\n",
    "                # Obtener los puntos que pertenecen al cluster actual\n",
    "                cluster_indices = np.where(cluster_labels_hdbscan == cluster_label)[0]\n",
    "                cluster_points = embedding[cluster_indices]\n",
    "                if cluster_points.shape[0] < 3:\n",
    "                    continue  # No se puede calcular el ConvexHull con menos de 3 puntos\n",
    "                # Calcular el ConvexHull\n",
    "                hull = ConvexHull(cluster_points)\n",
    "                # Crear el polígono\n",
    "                polygon = Polygon(\n",
    "                    cluster_points[hull.vertices],\n",
    "                    facecolor=polygon_color,\n",
    "                    edgecolor='none',\n",
    "                    alpha=polygon_alpha\n",
    "                )\n",
    "                plt.gca().add_patch(polygon)\n",
    "                # Calcular el centroide del cluster\n",
    "                centroid = cluster_points.mean(axis=0)\n",
    "                # Graficar el centroide con un punto rojo de tamaño 5\n",
    "                plt.scatter(\n",
    "                    centroid[0],\n",
    "                    centroid[1],\n",
    "                    c='red',\n",
    "                    s=50,\n",
    "                    marker='x',\n",
    "                    label='Centroides' if cluster_label == unique_clusters_hdbscan[0] else \"\"\n",
    "                )\n",
    "            \n",
    "            plt.title(f'UMAP(nn={n_neighbors}, md={min_dist}) + HDBSCAN(mcs={min_cluster_size}, ms={min_samples})')\n",
    "            plt.xlabel('UMAP 1')\n",
    "            plt.ylabel('UMAP 2')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            # Guardar la figura\n",
    "            plt.savefig(os.path.join(hdbscan_dir, \"hdbscan_plot.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # Guardar imágenes de centroides para HDBSCAN sin mostrar\n",
    "        centroids_dir = os.path.join(hdbscan_dir, \"centroids\")\n",
    "        os.makedirs(centroids_dir, exist_ok=True)\n",
    "        for cluster_label in unique_clusters_hdbscan:\n",
    "            cluster_indices = np.where(cluster_labels_hdbscan == cluster_label)[0]\n",
    "            # En caso de que no haya puntos en el cluster\n",
    "            if len(cluster_indices) == 0:\n",
    "                continue\n",
    "            cluster_points = embedding[cluster_indices]\n",
    "            # Calcular el centroide del cluster en el espacio embebido\n",
    "            centroid = cluster_points.mean(axis=0)\n",
    "            # Calcular la distancia de cada punto del cluster al centroide\n",
    "            distances_to_centroid = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "            # Encontrar el índice del punto más cercano al centroide\n",
    "            closest_point_idx = cluster_indices[np.argmin(distances_to_centroid)]\n",
    "            # Obtener el nombre del archivo de imagen correspondiente\n",
    "            image_filename = image_files[closest_point_idx]\n",
    "            image_path = os.path.join(images_directory, image_filename)\n",
    "            # Cargar y guardar la imagen\n",
    "            image = Image.open(image_path)\n",
    "            image.convert('L').save(os.path.join(centroids_dir, f\"cluster_{cluster_label}_centroid.png\"))\n",
    "    \n",
    "    # ----------------------\n",
    "    # Clustering con DBSCAN\n",
    "    # ----------------------\n",
    "    # We apply DBSCAN clustering on the UMAP-reduced data using different parameter combinations.\n",
    "\n",
    "    for eps_value, min_samples in dbscan_params:\n",
    "        print(f\"  Procesando DBSCAN con eps={eps_value}, min_samples={min_samples}\")\n",
    "        # Aplicar DBSCAN con los parámetros actuales\n",
    "        dbscan_clusterer = DBSCAN(eps=eps_value, min_samples=min_samples)\n",
    "        cluster_labels_dbscan = dbscan_clusterer.fit_predict(embedding_scaled)\n",
    "        \n",
    "        # Calcular métricas de clustering\n",
    "        num_clusters = len(set(cluster_labels_dbscan)) - (1 if -1 in cluster_labels_dbscan else 0)\n",
    "        if num_clusters > 1:\n",
    "            silhouette_avg = silhouette_score(embedding_scaled[cluster_labels_dbscan != -1],\n",
    "                                              cluster_labels_dbscan[cluster_labels_dbscan != -1])\n",
    "        else:\n",
    "            silhouette_avg = np.nan\n",
    "        \n",
    "        if num_clusters > 1:\n",
    "            davies_bouldin = davies_bouldin_score(embedding_scaled[cluster_labels != -1], \n",
    "                                                  cluster_labels[cluster_labels != -1])\n",
    "        else:\n",
    "            davies_bouldin = np.nan\n",
    "\n",
    "        if num_clusters > 1:\n",
    "            calinski_harabasz = calinski_harabasz_score(embedding_scaled[cluster_labels != -1], \n",
    "                                                        cluster_labels[cluster_labels != -1])\n",
    "        else:\n",
    "            calinski_harabasz = np.nan\n",
    "        \n",
    "        \n",
    "        # Guardar métricas en la lista\n",
    "        metrics_list.append({\n",
    "            'UMAP_n_neighbors': n_neighbors,\n",
    "            'UMAP_min_dist': min_dist,\n",
    "            'UMAP_n_components': n_components,\n",
    "            'Algorithm': 'DBSCAN',\n",
    "            'Params': f\"eps={eps_value}, min_samples={min_samples}\",\n",
    "            'Num_Clusters': num_clusters,\n",
    "            'Silhouette_Score': silhouette_avg,\n",
    "            'Davies_Bouldin_Index': davies_bouldin,\n",
    "            'Calinski_Harabasz_Index': calinski_harabasz\n",
    "        })\n",
    "        \n",
    "        # Crear directorio para esta combinación de parámetros DBSCAN\n",
    "        dbscan_dir_name = f\"DBSCAN_eps{eps_value}_ms{min_samples}\"\n",
    "        dbscan_dir = os.path.join(umap_dir, dbscan_dir_name)\n",
    "        os.makedirs(dbscan_dir, exist_ok=True)\n",
    "        \n",
    "        # Guardar las etiquetas de cluster\n",
    "        np.save(os.path.join(dbscan_dir, \"cluster_labels.npy\"), cluster_labels_dbscan)\n",
    "        \n",
    "        # Si n_components es 2, podemos generar gráficos\n",
    "        if n_components == 2:\n",
    "            # Visualizar y guardar el gráfico sin mostrarlo\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            # Graficar los puntos de ruido por separado\n",
    "            noise_mask_dbscan = cluster_labels_dbscan == -1\n",
    "            plt.scatter(\n",
    "                embedding[noise_mask_dbscan, 0],\n",
    "                embedding[noise_mask_dbscan, 1],\n",
    "                c='black',\n",
    "                s=10,\n",
    "                alpha=0.5,\n",
    "                label='Ruido'\n",
    "            )\n",
    "            \n",
    "            # Graficar los puntos asignados a clusters\n",
    "            cluster_mask_dbscan = ~noise_mask_dbscan\n",
    "            scatter_dbscan = plt.scatter(\n",
    "                embedding[cluster_mask_dbscan, 0],\n",
    "                embedding[cluster_mask_dbscan, 1],\n",
    "                c=indices[cluster_mask_dbscan],\n",
    "                cmap='viridis',\n",
    "                s=30,\n",
    "                alpha=0.8,\n",
    "                label='Datos'\n",
    "            )\n",
    "            plt.colorbar(scatter_dbscan, label='Índice (Tiempo)')\n",
    "            \n",
    "            # Generar y dibujar los polígonos para los clusters\n",
    "            unique_clusters_dbscan = np.unique(cluster_labels_dbscan)\n",
    "            unique_clusters_dbscan = unique_clusters_dbscan[unique_clusters_dbscan != -1]  # Excluir el ruido\n",
    "            \n",
    "            for cluster_label in unique_clusters_dbscan:\n",
    "                # Obtener los puntos que pertenecen al cluster actual\n",
    "                cluster_indices = np.where(cluster_labels_dbscan == cluster_label)[0]\n",
    "                cluster_points = embedding[cluster_indices]\n",
    "                if cluster_points.shape[0] < 3:\n",
    "                    continue  # No se puede calcular el ConvexHull con menos de 3 puntos\n",
    "                # Calcular el ConvexHull\n",
    "                hull = ConvexHull(cluster_points)\n",
    "                # Crear el polígono\n",
    "                polygon = Polygon(\n",
    "                    cluster_points[hull.vertices],\n",
    "                    facecolor=polygon_color,\n",
    "                    edgecolor='none',\n",
    "                    alpha=polygon_alpha\n",
    "                )\n",
    "                plt.gca().add_patch(polygon)\n",
    "                # Calcular el centroide del cluster\n",
    "                centroid = cluster_points.mean(axis=0)\n",
    "                # Graficar el centroide con un punto rojo de tamaño 5\n",
    "                plt.scatter(\n",
    "                    centroid[0],\n",
    "                    centroid[1],\n",
    "                    c='red',\n",
    "                    s=50,\n",
    "                    marker='x',\n",
    "                    label='Centroides' if cluster_label == unique_clusters_dbscan[0] else \"\"\n",
    "                )\n",
    "            \n",
    "            plt.title(f'UMAP(nn={n_neighbors}, md={min_dist}) + DBSCAN(eps={eps_value}, ms={min_samples})')\n",
    "            plt.xlabel('UMAP 1')\n",
    "            plt.ylabel('UMAP 2')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            # Guardar la figura\n",
    "            plt.savefig(os.path.join(dbscan_dir, \"dbscan_plot.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # Guardar imágenes de centroides para DBSCAN sin mostrar\n",
    "        centroids_dir = os.path.join(dbscan_dir, \"centroids\")\n",
    "        os.makedirs(centroids_dir, exist_ok=True)\n",
    "        for cluster_label in unique_clusters_dbscan:\n",
    "            cluster_indices = np.where(cluster_labels_dbscan == cluster_label)[0]\n",
    "            if len(cluster_indices) == 0:\n",
    "                continue\n",
    "            cluster_points = embedding[cluster_indices]\n",
    "            # Calcular el centroide del cluster\n",
    "            centroid = cluster_points.mean(axis=0)\n",
    "            # Calcular la distancia de cada punto del cluster al centroide\n",
    "            distances_to_centroid = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "            # Encontrar el índice del punto más cercano al centroide\n",
    "            closest_point_idx = cluster_indices[np.argmin(distances_to_centroid)]\n",
    "            # Obtener el nombre del archivo de imagen correspondiente\n",
    "            image_filename = image_files[closest_point_idx]\n",
    "            image_path = os.path.join(images_directory, image_filename)\n",
    "            # Cargar y guardar la imagen\n",
    "            image = Image.open(image_path)\n",
    "            image.convert('L').save(os.path.join(centroids_dir, f\"cluster_{cluster_label}_centroid.png\"))\n",
    "\n",
    "# Crear el DataFrame de métricas a partir de la lista\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "# Guardar las métricas de clustering en un archivo CSV\n",
    "metrics_df.to_csv(os.path.join(results_base_dir, \"clustering_metrics.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b2b68-556e-4421-b5ac-e0ab71a7c972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
